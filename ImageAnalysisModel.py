import ContainerScalerModel as cs
import sizeAnalysisModel as sa
import ImageProcessingModel as ip
import logger_config
import ParticleSegmentationModel as psa
logger = logger_config.get_logger(__name__)
import os
import re
import csv
import math
import bisect
import configparser
# if using Apple MPS, fall back to CPU for unsupported ops
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"

# -----------------------------------------------------------------------------
# ImageAnalysisModel Class
# -----------------------------------------------------------------------------
# The ImageAnalysisModel class serves as the central interface for analyzing
# images and extracting particle-related data. It integrates multiple models
# and utilities for preprocessing, scaling, segmentation, and analysis.
#
# Core Features:
# 1. Initialization:
#    - Takes the path to a folder containing images and a container width.
#    - Automatically sets up the sample ID, image processor, and scaling factor.
#
# 2. Preprocessing:
#    - Allows cropping and lighting adjustments to improve image quality.
#    - Supports overlaying images for enhanced visibility and size reduction.
#
# 3. Particle Segmentation and Analysis:
#    - Uses a ParticleSegmentationModel to segment particles and generate masks.
#    - Supports loading pretrained checkpoints for segmentation models.
#    - Analyzes particle size distribution (PSD) and saves data in various formats.
#
# 4. Results Management:
#    - Saves results such as particle masks, PSD data, and formatted XML files.
#    - Provides functionality to load pre-segmented data for analysis.
#
# 5. Visualization:
#    - Displays images and generated masks for inspection.
#
# 6. Customization:
#    - Supports adjustable bins and diameter thresholds for segmentation.
#
# Designed for extensibility and efficient image analysis, this class is
# structured to integrate with other models and tools seamlessly.
# -----------------------------------------------------------------------------


class ImageAnalysisModel:
    def __init__(self, image_folder_path, scalingNumber=None,containerWidth=None, sampleID=None):
        """
        Initializes the ImageAnalysisModel with an image folder path and container width.
        Sets up the sample ID, image processor, and container scaler.

        Inputs:
        - image_folder_path: Path to the folder containing images for analysis.
        - containerWidth: Width of the container used for scaling.

        Output: None
        """
        self.sampleID = sampleID if sampleID else os.path.basename(
            image_folder_path)
        self.imageProcessor = ip.ImageProcessingModel(
            image_folder_path, self.sampleID)
        self.imagePath = self.imageProcessor.getImagePath()
        self.meshingImageFolderPath=None
        self.Scaler = cs.ContainerScalerModel(containerWidth)
        self.Scaler.updateScalingFactor(
            imageWidth=self.imageProcessor.getWidth(),scalingNumber=scalingNumber, containerWidth=containerWidth)
        self.diameter_threshold = 100000  # 10cm
        self.folder_path = image_folder_path
        self.meshingTotalSeconds=0
        self.totalSeconds=0
        self.analysisTime = 0
        self.numberofBins = 0
        self.p = None
        self.csv_filename = ""
        self.totalSecondes=0
        self.minimumArea=0
        self.meshingSegmentAreas={}
        self.miniParticles=[]
        self.UnSegmentedArea = None
        self.particles=[]
        self.csv_filename=""

    def analysewithCV2(self):
        self.csv_filename = os.path.join(
            self.folder_path, f"{self.sampleID}.csv")
        self.p.generate_with_cv2(self.csv_filename)

    def showImage(self):
        """
        Displays the processed image using the ImageProcessingModel.

        Input: None
        Output: Shows the image.
        """
        self.imageProcessor.showImage()

    def showMasks(self):
        """
        Displays the masks generated by the ParticleSegmentationModel, if available.

        Input: None
        Output: Shows mask visualization.
        """
        file_name = f"{self.folder_path}/{self.sampleID}_mask.png"
        self.p.visualise_masks(file_name)

    def setBins(self, bins):
        """
        Sets the number of bins in the ParticleSegmentationModel based on input.

        Inputs:
        - bins: List of bin boundaries.

        Output: None
        """
        if self.p is not None:
            self.numberofBins = len(bins)
            self.p.bins = bins[:]

    def loadModel(self, checkpoint_folder):
        """
        Loads the ParticleSegmentationModel with a specified checkpoint.

        Input:
        - checkpoint_folder: Path to the folder containing model checkpoint.

        Output: None
        """
        def loadSamModel(checkpoint_folder):
            os.makedirs(checkpoint_folder, exist_ok=True)
            checkpoint_filename = "sam2.1_hiera_large.pt"  # SAM2
            CHECKPOINT_PATH = os.path.join(
                checkpoint_folder, checkpoint_filename)
            return CHECKPOINT_PATH

        CHECKPOINT_PATH = loadSamModel(checkpoint_folder)
        self.p = psa.ParticleSegmentationModel(
            self.imagePath, CHECKPOINT_PATH, self.Scaler.scalingFactor)

    def analyseParticles(self, checkpoint_folder, testing):
        """
        Analyzes particles in the image by generating masks using the model, and calculates analysis time.

        Inputs:
        - checkpoint_folder: Path to the model checkpoint.
        - testing: Boolean flag to enable test mode.

        Output: None
        """
        def calculateAnalysisTime(duration):
            total_seconds = duration.total_seconds()
            # Total seconds for the first calculation (Entire image)
            # self.totalSeconds=total_seconds
            minutes = int(total_seconds // 60)
            seconds = total_seconds % 60
            self.analysisTime = f"PT{minutes}M{seconds:.1f}S"

        self.loadModel(checkpoint_folder)
        if testing:
            self.p.testing_generate_mask()
        else:
            self.p.generate_mask()

        calculateAnalysisTime(self.p.getExecutionTime())
        self.p.setdiameter_threshold(self.diameter_threshold)
        self.csv_filename = os.path.join(
            self.folder_path, f"{self.sampleID}.csv")
        self.p.save_masks_to_csv(self.csv_filename)
        self.showMasks()
    def analyseValidationParticles(self, checkpoint_folder,parameter_folder_name, testing_parameters=None):
        """
        Analyzes particles in the image by generating masks using the model, and calculates analysis time.

        Inputs:
        - checkpoint_folder: Path to the model checkpoint.
        - testing: Boolean flag to enable test mode.

        Output: None
        """
        def calculateAnalysisTime(duration):
            total_seconds = duration.total_seconds()
            # Total seconds for the first calculation (Entire image)
            # self.totalSeconds=total_seconds
            minutes = int(total_seconds // 60)
            seconds = total_seconds % 60
            self.analysisTime = f"PT{minutes}M{seconds:.1f}S"

        self.loadModel(checkpoint_folder)

        self.p.testing_generate_mask_1(**testing_parameters)


        calculateAnalysisTime(self.p.getExecutionTime())
        self.p.setdiameter_threshold(self.diameter_threshold)
        # Get Image folder Path
        original_folder_path = self.imageProcessor.getImageFolder()
        # Create subfolder for the current parameter set
        self.folder_path = os.path.join(original_folder_path, parameter_folder_name)
        os.makedirs(self.folder_path, exist_ok=True)
        self.csv_filename = os.path.join(
            self.folder_path, f"{self.sampleID}.csv")
        self.p.save_masks_to_csv(self.csv_filename)
        self.showMasks()

    def savePsdData(self):
        """
        Saves particle size distribution (PSD) data to a text file.

        Input: None
        Output: Saves PSD data to a TXT file.
        """
        self.p.get_psd_data()
        self.distributions_filename = os.path.join(
            self.folder_path, f"{self.sampleID}_byArea_distribution.txt")
        self.p.save_psd_as_txt(self.sampleID, self.distributions_filename)
        print(f"--> PSD data saved as TXT file: {self.distributions_filename}")
    def savePsdDataWithDiameter(self):
        """
        Saves particle size distribution (PSD) data to a text file.

        Input: None
        Output: Saves PSD data to a TXT file.
        """
        self.p.get_psd_data_with_diameter()
        self.distributions_filename = os.path.join(
            self.folder_path, f"{self.sampleID}_bySize_distribution.txt")
        self.p.save_psd_as_txt(self.sampleID, self.distributions_filename)
        print(f"--> PSD data saved as TXT file: {self.distributions_filename}")

    def savePsdDataForNormalBins(self):
        """
        Saves particle size distribution (PSD) data to a text file.

        Input: None
        Output: Saves PSD data to a TXT file.
        """
        self.p.get_psd_data()
        # self.distributions_filename = os.path.join(
        #     self.folder_path, f"{self.sampleID}_normalBin_distribution.txt")
        self.p.save_psd_as_txt_normal(self.sampleID, self.folder_path)
        # print(f"--> PSD data saved as TXT file: {self.distributions_filename}")
    def saveDistributionPlot(self):
        """
        Saves particle size distribution (PSD) data to a text file.

        Input: None
        Output: Saves PSD data to a TXT file.
        """

        self.p.plotBins(self.folder_path,self.sampleID)

    def saveDistributionPlotForDiameter(self):
        """
        Saves particle size distribution (PSD) data to a text file.

        Input: None
        Output: Saves PSD data to a TXT file.
        """

        self.p.plotBinsForDiameter(self.folder_path, self.sampleID)
    def saveDistributionPlotForNormalBins(self):
        """
        Saves particle size distribution (PSD) data to a text file.

        Input: None
        Output: Saves PSD data to a TXT file.
        """
        self.p.plotNormalBins(self.folder_path, self.sampleID)



    def saveResults(self, bins):
        """
        Saves particle segmentation results to CSV and distribution files after setting bins.

        Input:
        - bins: List of bin boundaries for the segmentation model.

        Output: Saves results to CSV and distribution files.
        """
        self.setBins(bins)
        if self.imageProcessor is None:
            raise ValueError("Image is not initialised")

        self.folder_path = self.imageProcessor.getImageFolder()
        self.csv_filename = os.path.join(
            self.folder_path, f"{self.sampleID}.csv")
        self.p.setdiameter_threshold(self.diameter_threshold)
        self.p.save_masks_to_csv(self.csv_filename)
        print(f"--> Masks saved to CSV file: {self.csv_filename}")

        self.savePsdData()
        self.saveDistributionPlot()

    def saveResultsForValidation(self, bins, parameter_folder_name):
        """
        Saves particle segmentation results to CSV and distribution files after setting bins.

        Input:
        - bins: List of bin boundaries for the segmentation model.
        - parameter_folder_name: Name of the subfolder for the current parameter set.

        Output: Saves results to CSV and distribution files in the specified subfolder.
        """
        self.setBins(bins)
        if self.imageProcessor is None:
            raise ValueError("Image is not initialised")



        # Generate new csv
        self.csv_filename = os.path.join(self.folder_path, f"{self.sampleID}.csv")
        self.p.setdiameter_threshold(self.diameter_threshold)
        self.p.save_masks_to_csv(self.csv_filename)
        print(f"--> Masks saved to CSV file: {self.csv_filename}")

        self.savePsdData()
        self.saveDistributionPlot()
    def saveResultsForNormalBinsOnly(self, bins):
        """
        Saves particle segmentation results to CSV and distribution files after setting bins.

        Input:
        - bins: List of bin boundaries for the segmentation model.

        Output: Saves results to CSV and distribution files.
        """
        self.setBins(bins)
        self.savePsdDataForNormalBins()
        self.saveDistributionPlotForNormalBins()
    def generateMasksForMeshing(self, testing):
        """
        Analyzes particles in the image by generating masks using the model for each segmented image
        and saves the resulting segment files to a corresponding folder.

        Inputs:
        - testing: Boolean flag to enable test mode.

        Output: None
        """

        def calculateTotalSeconds(duration):
            total_seconds = duration.total_seconds()
            self.meshingTotalSeconds += total_seconds

        # Step 1: Assign `self.meshingImageFolderPath`
        meshing_folder_name = "meshingImage"
        self.meshingImageFolderPath = os.path.join(self.folder_path, meshing_folder_name)

        # Step 2: Check if `meshingImage` folder exists
        if not os.path.exists(self.meshingImageFolderPath):
            print(f"Error: Folder '{meshing_folder_name}' not found at {self.folder_path}")
            return

        print(f"Meshing image folder path: {self.meshingImageFolderPath}")

        # Step 3: Assign `self.meshingSegmentsFolder`
        meshing_segment_folder_name = "meshingSegments"
        self.meshingSegmentsFolder = os.path.join(self.folder_path, meshing_segment_folder_name)

        # Create `meshingSegments` folder if it doesn't exist
        os.makedirs(self.meshingSegmentsFolder, exist_ok=True)
        print(f"Meshing segments folder path: {self.meshingSegmentsFolder}")

        # Step 4: List all image files in `meshingImageFolderPath` with natural sorting
        def natural_key(file_name):
            match = re.search(r'\d+', file_name)
            return int(match.group()) if match else float('inf')

        image_files = [
            os.path.join(self.meshingImageFolderPath, file)
            for file in sorted(os.listdir(self.meshingImageFolderPath),key=natural_key)
            if file.endswith(".png")  # Filter for PNG images
        ]

        if not image_files:
            print(f"No images found in {self.meshingImageFolderPath}")
            return

        print(f"Found {len(image_files)} images in {self.meshingImageFolderPath}")

        # Step 5: Loop through each image and process using the model
        for index, image_path in enumerate(image_files, start=1):
            print(f"Processing image: {image_path}")

            # Update the model's image path directly
            self.p.update_image_path(image_path)

            # Generate masks
            if testing:
                self.p.testing_generate_mask()
            else:
                self.p.generate_mask()

            # Step 6: Save masks to a corresponding CSV file
            self.p.setdiameter_threshold(self.diameter_threshold)
            csv_filename = os.path.join(self.meshingSegmentsFolder, f"meshing_{index}.csv")
            self.p.save_masks_to_csv(csv_filename)
            print(f"Segment file saved as: {csv_filename}")
            self.showMasks()

            # Calculate execution time
            calculateTotalSeconds(self.p.getExecutionTime())

        print("Finished processing all images.")

    def setScalingFactor(self, scalingFactor):
        self.Scaler.setScalingFactor(scalingFactor)

    def formatResults(self,byArea=False,bySize=False):
        """
        Formats and displays analysis results, and saves formatted results as XML.

        Input: None
        Output: Prints formatted results and saves them to an XML file.
        """
        self.totArea = self.p.get_totalArea()
        print("-----------------------------------------------")
        print("Sample ID:", self.sampleID)
        print(f"Total Area: {self.totArea} um2")
        print(f"Total Area: {self.totArea / 100_000_000} cm2")
        print(f"Scaling Factor: {self.Scaler.scalingFactor} um/pixels")
        print(f"Scaling Number: {self.Scaler.scalingNumber} pixels")
        self.intensity = self.imageProcessor.getIntensity()
        print("Intensity:", self.intensity)
        print("Scaling Stamp:", self.Scaler.scalingStamp)
        print("Analysis Time:", self.analysisTime)
        print(f"Diameter Threshold: {self.p.diameter_threshold} um")
        print(f"Circularity Threshold: {self.p.circularity_threshold} um")
        print("-----------------------------------------------")
        print(f"CSV file: {self.csv_filename}")

        formatter = sa.sizeAnalysisModel(self.sampleID, self.csv_filename, self.distributions_filename,
                                         self.totArea, self.Scaler.scalingNumber,
                                         self.Scaler.scalingFactor, self.Scaler.scalingStamp,
                                         self.intensity, self.analysisTime, self.p.diameter_threshold,
                                         self.p.circularity_threshold)
        formatter.save_xml(byArea=byArea,bySize=bySize)

    def formatResultsForNormalDistribution(self,normalFlag):
        """
        Formats and displays analysis results, and saves formatted results as XML.

        Input: None
        Output: Prints formatted results and saves them to an XML file.
        """
        self.totArea = self.p.get_totalArea()
        print("-----------------------------------------------")
        print("Sample ID:", self.sampleID)
        print(f"Total Area: {self.totArea} um2")
        print(f"Total Area: {self.totArea / 100_000_000} cm2")
        print(f"Scaling Factor: {self.Scaler.scalingFactor} um/pixels")
        print(f"Scaling Number: {self.Scaler.scalingNumber} pixels")
        self.intensity = self.imageProcessor.getIntensity()
        print("Intensity:", self.intensity)
        print("Scaling Stamp:", self.Scaler.scalingStamp)
        print("Analysis Time:", self.analysisTime)
        print(f"Diameter Threshold: {self.p.diameter_threshold} um")
        print(f"Circularity Threshold: {self.p.circularity_threshold} um")
        print("-----------------------------------------------")
        print(f"CSV file: {self.csv_filename}")
        normalBins_distributions_filename = os.path.join(
            self.folder_path, f"{self.sampleID}_normalBin_distribution.txt")
        formatter = sa.sizeAnalysisModel(self.sampleID, self.csv_filename, normalBins_distributions_filename,
                                         self.totArea, self.Scaler.scalingNumber,
                                         self.Scaler.scalingFactor, self.Scaler.scalingStamp,
                                         self.intensity, self.analysisTime, self.p.diameter_threshold,
                                         self.p.circularity_threshold)
        formatter.save_xml(normalFlag=normalFlag)

    def saveSegments(self):
        """
        Saves segment data as JSON for later use.

        Input: None
        Output: Saves segment data to JSON file.
        """
        self.p.setdiameter_threshold(self.diameter_threshold)
        self.json_filename = os.path.join(
            self.folder_path, f"{self.sampleID}_segments.txt")
        self.p.save_segments(self.json_filename)
        print(f"Saving segments in {self.json_filename}")


    def loadSegments(self, checkpoint_folder, bins):
        """
        Loads segments from a JSON file and saves them to CSV and distribution files, useful for non-GPU environments.

        Inputs:
        - checkpoint_folder: Path to the model checkpoint.
        - bins: List of bin boundaries for the segmentation model.

        Output: Saves segment data to CSV and distribution files.
        """
        try:
            self.setFolderPath()
            self.json_masks_filename = os.path.join(
                self.folder_path, f"{self.sampleID}_segments.txt")

            if not os.path.exists(self.json_masks_filename):
                raise FileNotFoundError(
                    f"The file {self.json_masks_filename} was not found.")

            self.loadModel(checkpoint_folder)
            self.setBins(bins)
            self.csv_filename = os.path.join(
                self.folder_path, f"{self.sampleID}.csv")
            self.p.setdiameter_threshold(self.diameter_threshold)
            self.p.save_segments_as_csv(
                self.json_masks_filename, self.csv_filename)
            # self.savePsdData()
            # self.savePsdDataWithDiameter()

        except FileNotFoundError as e:
            raise e
        except Exception as e:
            raise Exception(f"An unexpected error occurred: {e}")

    def setFolderPath(self):
        """
        Sets the folder path for saving results, based on the initialized image processor.

        Input: None
        Output: Sets self.folder_path based on image folder path.
        """
        if self.imageProcessor is not None:
            self.folder_path = self.imageProcessor.getImageFolder()
        else:
            raise ValueError(
                "Image not initialized. Please ensure that 'imageProcessor' is properly initialized.")

    def crop_image(self):
        self.imageProcessor.cropImage()
        self.imagePath = self.imageProcessor.getImagePath()
        self.Scaler.updateScalingFactor(self.imageProcessor.getWidth())

    def evenLighting(self):
        self.imageProcessor.even_out_lighting()
        self.imagePath = self.imageProcessor.getImagePath()
        self.Scaler.updateScalingFactor(self.imageProcessor.getWidth())

    def evenLightingWithValidation(self,parameter_folder_path):
        self.imageProcessor.even_out_lighting_validation(parameter_folder_path)
        # self.imagePath = self.imageProcessor.getImagePath()


    def overlayImage(self):
        """
        Calls the ImageProcessingModel's overlayImage function to overlay the same picture 10 times and
        reducing the size of the image if it is bigger than 8MB

        Input: None
        Output: lighter PNG file and containing the same image overlayed 10 times
        """
        self.imageProcessor.overlayImage()
        self.imagePath = self.imageProcessor.getImagePath()
        self.Scaler.updateScalingFactor(self.imageProcessor.getWidth())
    def overlayImageWithValidation(self):
        """
        Calls the ImageProcessingModel's overlayImage function to overlay the same picture 10 times and
        reducing the size of the image if it is bigger than 8MB

        Input: None
        Output: lighter PNG file and containing the same image overlayed 10 times
        """
        self.imageProcessor.overlayImage()
        self.imagePath = self.imageProcessor.getImagePath()



    def  meshingImage(self):
        self.imageProcessor.processImageWithMeshing()


    def plotBins(self):
        self.p.plotBins()

    def getAnalysisTime(self):
        total_seconds = self.totalSeconds+self.meshingTotalSeconds
        # Total seconds for the first calculation (Entire image)
        minutes = int(total_seconds // 60)
        seconds = total_seconds % 60
        self.analysisTime = f"PT{minutes}M{seconds:.1f}S"
        print(f"""The final analysing time is {self.analysisTime}""")
    def getSmallestAreaForFinalImage(self):
        """
        This function counts the number of data rows in a given file, ignoring the header row.

        Args:
        file_path (str): The path to the file.

        Returns:
        int: The number of data rows in the file.
        """
        particles=[]
        try:
            with open(self.csv_filename, 'r') as file:
                next(file)
                for line in file:
                    if line.strip():  # remove white space
                        area, perimeter, diameter, circularity = map(float, line.strip().split(','))
                        item = {
                            "area": area,
                            "perimeter": perimeter,
                            "diameter": diameter,
                            "circularity": circularity
                        }
                        particles.append(item)
            if len(particles) == 0:
                logger.error("There is no particles for minimumArea(ImageAnalysisModel) to be processed")
                return

            areas = [particle['area'] for particle in particles]
            sorted_areas = sorted(areas)
            self.mimumArea = format(max(float(sorted_areas[0] / 1000000), 0), '.8f')
            print(f'Minimu Area(ImageAnalysisModel) of the entire image analysis is :{self.mimumArea}')
            logger.info("Minimu Area(ImageAnalysisModel) of the entire image analysis is : {}", self.mimumArea)

        except Exception as e :
                logger.error("The give csv  file can  not be parsed due to {} error ",e)

    def getMeshingSegmentByCompareAreas(self):
        """
        Processes all CSV files in `self.meshingSegmentsFolder` and extracts particle areas.
        Compares each area with self.minimumArea and stores particles with areas smaller than self.minimumArea.

        Args:
        None

        Returns:
        None
        """
        # Ensure self.minimumArea is initialized
        if not hasattr(self, 'minimumArea'):
            self.minimumArea = float('inf')  # Set a large initial value if not set


        # Check if the folder exists
        if not os.path.exists(self.meshingSegmentsFolder):
            logger.error(f"Meshing segments folder {self.meshingSegmentsFolder} does not exist.")
            return

        # Get all CSV files in the folder
        segment_files = [
            os.path.join(self.meshingSegmentsFolder, file)
            for file in os.listdir(self.meshingSegmentsFolder)
            if file.endswith('.csv')
        ]

        if not segment_files:
            logger.error(f"No CSV files found in {self.meshingSegmentsFolder}")
            return

        # Process each CSV file
        for segment_file in sorted(segment_files):
            particles = []
            try:
                with open(segment_file, 'r') as file:
                    next(file)  # Skip the header row
                    for line in file:
                        if line.strip():  # Remove white space
                            # Parse the row
                            area, perimeter, diameter, circularity = map(float, line.strip().split(','))
                            formatted_area = format(max(float(area / 1000000), 0), '.8f')
                            item = {
                                "area": area,  # Store the original area
                                "perimeter": perimeter,
                                "diameter": diameter,
                                "circularity": circularity
                            }
                            # Compare formatted_area and not the original area
                            if float(formatted_area) < float(self.minimumArea):
                                self.miniParticles.append(item)
                            particles.append(item)

                if not particles:
                    logger.warning(f"No particles found in file {segment_file}")
                    continue

                print(f"Processed file {segment_file}: {len(particles)} particles")
                logger.info(f"Processed file {segment_file}: {len(particles)} particles")

            except Exception as e:
                logger.error(f"Failed to process file {segment_file} due to error: {e}")

        if not self.miniParticles:
            logger.warning("No particles smaller than minimumArea were found.")
        else:
            logger.info(f"Found {len(self.miniParticles)} particles smaller than minimumArea.")

    def processingMiniParticles(self):
        """
        Check the `self.miniParticles` list and write its contents to a new CSV file along with the contents of another existing CSV file.

        Args:
        None

        Returns:
        None
        """

        final_csv_path = os.path.join(self.folder_path, f"final_{self.sampleID}.csv")
        original_csv_path = os.path.join(self.folder_path, f"{self.sampleID}.csv")

        # Check if miniParticles is not empty
        if self.miniParticles:
            with open(final_csv_path, 'w', newline='') as csvfile:
                fieldnames = ['area', 'perimeter', 'diameter', 'circularity']
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

                # Write the header
                writer.writeheader()

                # Write miniParticles to the new CSV
                for particle in self.miniParticles:
                    writer.writerow(particle)

                # Check if the original CSV exists and append its contents
                if os.path.exists(original_csv_path):
                    with open(original_csv_path, 'r') as original_csvfile:
                        reader = csv.DictReader(original_csvfile)
                        for row in reader:
                            writer.writerow(row)
                else:
                    print(f"Original CSV file {original_csv_path} does not exist.")
        else:
            print("No mini particles to process.")

    def save_final_results(self,bins):
        self.setBins(bins)
        self.p.setdiameter_threshold(self.diameter_threshold)
        final_csv = os.path.join(self.folder_path, f"final_{self.sampleID}.csv")
        regular_csv = os.path.join(self.folder_path, f"{self.sampleID}.csv")

        # Determine which file exists and set the appropriate output txt filename
        if os.path.exists(final_csv):
            input_file = final_csv
            output_txt = os.path.join(self.folder_path, "final_mesh_segments.txt")
        elif os.path.exists(regular_csv):
            input_file = regular_csv
            output_txt = os.path.join(self.folder_path, "final_segment.txt")
        else:
            print("No appropriate CSV file found.")
            return

        # Convert the CSV to TXT in JSON format
        self.convert_csv_to_json_txt(input_file, output_txt)

        # Assuming self.p has an open_segments method
        self.p.open_segments(output_txt)

        # Assuming a method to save PSD data
        self.savePsdData()

    def convert_csv_to_json_txt(self, csv_file_path, json_txt_output_path):
        """
        Converts a CSV file to a JSON-like format in plain text.

        Args:
        csv_file_path (str): Path to the input CSV file.
        json_txt_output_path (str): Path to the output text file with JSON format.

        Returns:
        None
        """
        try:
            data_list = []
            with open(csv_file_path, mode='r', newline='') as file:
                reader = csv.DictReader(file)
                for row in reader:
                    # build String
                    line = "{\n" + ",\n".join(f"    {k}: {v}" for k, v in row.items()) + "\n},"
                    data_list.append(line)

            with open(json_txt_output_path, 'w') as output_file:
                # write to the csv file
                output_file.write("[\n" + ",\n".join(data_list)[:-1] + "\n]")
            print(f"Data successfully written to {json_txt_output_path}")

        except Exception as e:
            print(f"An error occurred while converting CSV to TXT: {e}")

    def getTheCalculatedBinsBySize(self,target_distribution=None):
        """
        Calculate the adjusted bins aligns with lab result
        Args:

            segmentsFilePath: The segement.txt file path
            target_distribution: Laboratory Screen Percentages (target distribution).
        Returns:
            bins: Adjusted bins that will fit the Laboratory Screen Percentages
        """



        if len(self.particles)==0:

            if not os.path.exists(self.csv_filename):
                return
            with open(self.csv_filename, 'r') as file:
                next(file)
                for line in file:
                    if line.strip():  # remove white space
                        area, perimeter, diameter, circularity = map(float, line.strip().split(','))
                        item = {
                            "area": area,
                            "perimeter": perimeter,
                            "diameter": diameter,
                            "circularity": circularity
                        }
                        self.particles.append(item)
        if len(self.particles)==0:
            return
        cumulative_percentage = 0.0
        diameters = [particle['diameter'] for particle in self.particles]
        diameters = sorted(diameters, reverse=True)
        # Sort diameters in ascending order
        total_particles = len(diameters)
        bins = []

        for percentage in target_distribution[:-1]:  # The last bin does not require additional calculation
            cumulative_percentage += percentage
            index = int(
                cumulative_percentage / 100 * total_particles)  # Locate the index corresponding to the target percentage
            bin_value = diameters[min(index, total_particles - 1)]  # Ensure the index does not go out of bounds
            bins.append(math.ceil(bin_value))
        # Step 4: Add the maximum value of the last bin

        # Save the bins to a text file
        bins_file_path = os.path.join(self.folder_path, f"{self.sampleID}_bins_calibrated_size.txt")
        with open(bins_file_path, 'w') as file:
            file.write(', '.join(map(str, bins)))



    def calculate_cumulative_bins_byArea(self,target_distribution=None):
        """
        Calculate the diameters for cumulative percentage thresholds.

        Args:
            particles (list of dicts): List containing particle data with 'diameter' and 'area'.
            total_area (float): The total area of all particles.
            target_percentages (list): List of target percentages for cumulative calculations.

        Returns:
            dict: Dictionary where keys are cumulative percentages and values are the corresponding diameters.
        """
        if len(self.particles)==0:

            if not os.path.exists(self.csv_filename):
                return

            with open(self.csv_filename, 'r') as file:
                next(file)
                for line in file:
                    if line.strip():  # remove white space
                        area, perimeter, diameter, circularity = map(float, line.strip().split(','))
                        item = {
                            "area": area,
                            "perimeter": perimeter,
                            "diameter": diameter,
                            "circularity": circularity
                        }
                        self.particles.append(item)
        total_area = 0
        if len(self.particles) == 0:
            return
        areas = [particle['area'] for particle in self.particles]
        for area in areas:
            total_area += area

        bins=[]
        cumulative_percentage = 0.0

        for i, percentage in enumerate(target_distribution[:-1]):  # Exclude the last percentage
            cumulative_percentage += percentage
            diameter = self.find_cumulative_area_diameter(self.particles, total_area, cumulative_percentage)
            bins.append(diameter)
        bins_file_path = os.path.join(self.folder_path, f"{self.sampleID}_bins_calibrated_area.txt")
        with open(bins_file_path, 'w') as file:
            file.write(', '.join(map(str, bins)))

    def find_cumulative_area_diameter(self,particles, total_area, target_percentage):
        """
        Helper function to find the diameter at which a specific cumulative area percentage is reached.
        """
        target_area = total_area * (target_percentage / 100)
        particles_sorted = sorted(particles, key=lambda x: x['diameter'], reverse=True)
        cumulative_sum = 0.0

        for particle in particles_sorted:
            cumulative_sum += particle['area']
            if cumulative_sum >= target_area:
                return round(particle['diameter'])

        return None

    def calculate_unsegmented_area(self):
        config = configparser.ConfigParser()
        config.read('config.ini')

        # Extract containerWidth directly in micrometers (um)
        container_width_um = float(config['analysis']['containerWidth'])
        container_area_um2 = container_width_um ** 2  # Assuming the container is a square

        if self.totArea is not None:  # Check if totArea has been set
            if self.totArea < container_area_um2:
                self.UnSegmentedArea = container_area_um2 - self.totArea
            else:
                self.UnSegmentedArea = 0
        else:
            print("Total area (self.totArea) is not set.")

        print(f"Container Width (um): {container_width_um}")
        print(f"Container Area (um²): {container_area_um2}")
        print(f"Unsegmented Area (um²): {self.UnSegmentedArea}")

    def calculate_bins_with_unsegementedArea(self):

        if len(self.particles)==0:

            if not os.path.exists(self.csv_filename):
                return

            with open(self.csv_filename, 'r') as file:
                next(file)
                for line in file:
                    if line.strip():  # remove white space
                        area, perimeter, diameter, circularity = map(float, line.strip().split(','))
                        item = {
                            "area": area,
                            "perimeter": perimeter,
                            "diameter": diameter,
                            "circularity": circularity
                        }
                        self.particles.append(item)

        config = configparser.ConfigParser()
        config.read('config.ini')

        # Extract containerWidth directly in micrometers (um)

        standardBin = float(['analysis']['industryBin'])
        if not standardBin:
            return

        if len(self.particles) == 0:
            return
        minBin = standardBin[0]
        new_standardBin = []

        diameters = [particle['diameter'] for particle in self.particles]
        sorted_diameters = sorted(diameters)
        minimum_diameter = sorted_diameters[0]
        if minimum_diameter < minBin:
                minBin = round(minimum_diameter)
                new_standardBin = [minBin] + standardBin
        if minimum_diameter > minBin:
                bisect.insort(standardBin, round(minimum_diameter))
                new_standardBin = standardBin
        return new_standardBin
