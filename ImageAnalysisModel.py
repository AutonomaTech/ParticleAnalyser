import ContainerScalerModel as cs
import sizeAnalysisModel as sa
import ImageProcessingModel as ip
import logger_config
import ParticleSegmentationModel as psa
logger = logger_config.get_logger(__name__)
import os
import re
# if using Apple MPS, fall back to CPU for unsupported ops
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"

# -----------------------------------------------------------------------------
# ImageAnalysisModel Class
# -----------------------------------------------------------------------------
# The ImageAnalysisModel class serves as the central interface for analyzing
# images and extracting particle-related data. It integrates multiple models
# and utilities for preprocessing, scaling, segmentation, and analysis.
#
# Core Features:
# 1. Initialization:
#    - Takes the path to a folder containing images and a container width.
#    - Automatically sets up the sample ID, image processor, and scaling factor.
#
# 2. Preprocessing:
#    - Allows cropping and lighting adjustments to improve image quality.
#    - Supports overlaying images for enhanced visibility and size reduction.
#
# 3. Particle Segmentation and Analysis:
#    - Uses a ParticleSegmentationModel to segment particles and generate masks.
#    - Supports loading pretrained checkpoints for segmentation models.
#    - Analyzes particle size distribution (PSD) and saves data in various formats.
#
# 4. Results Management:
#    - Saves results such as particle masks, PSD data, and formatted XML files.
#    - Provides functionality to load pre-segmented data for analysis.
#
# 5. Visualization:
#    - Displays images and generated masks for inspection.
#
# 6. Customization:
#    - Supports adjustable bins and diameter thresholds for segmentation.
#
# Designed for extensibility and efficient image analysis, this class is
# structured to integrate with other models and tools seamlessly.
# -----------------------------------------------------------------------------


class ImageAnalysisModel:
    def __init__(self, image_folder_path, containerWidth, sampleID=None):
        """
        Initializes the ImageAnalysisModel with an image folder path and container width. 
        Sets up the sample ID, image processor, and container scaler.

        Inputs:
        - image_folder_path: Path to the folder containing images for analysis.
        - containerWidth: Width of the container used for scaling.

        Output: None
        """
        self.sampleID = sampleID if sampleID else os.path.basename(
            image_folder_path)
        self.imageProcessor = ip.ImageProcessingModel(
            image_folder_path, self.sampleID)
        self.imagePath = self.imageProcessor.getImagePath()
        self.meshingImageFolderPath=None
        self.Scaler = cs.ContainerScalerModel(containerWidth)
        self.Scaler.updateScalingFactor(
            self.imageProcessor.getWidth(), containerWidth)
        self.diameter_threshold = 100000  # 10cm
        self.folder_path = image_folder_path
        self.meshingTotalSeconds=0
        self.totalSeconds=0
        self.analysisTime = 0
        self.numberofBins = 0
        self.p = None
        self.csv_filename = ""
        self.totalSecondes=0
        self.mimumArea=0
        self.meshingSegmentAreas={}

    def analysewithCV2(self):
        self.csv_filename = os.path.join(
            self.folder_path, f"{self.sampleID}.csv")
        self.p.generate_with_cv2(self.csv_filename)

    def showImage(self):
        """
        Displays the processed image using the ImageProcessingModel.

        Input: None
        Output: Shows the image.
        """
        self.imageProcessor.showImage()

    def showMasks(self):
        """
        Displays the masks generated by the ParticleSegmentationModel, if available.

        Input: None
        Output: Shows mask visualization.
        """
        self.p.visualise_masks()

    def setBins(self, bins):
        """
        Sets the number of bins in the ParticleSegmentationModel based on input.

        Inputs:
        - bins: List of bin boundaries.

        Output: None
        """
        if self.p is not None:
            self.numberofBins = len(bins)
            self.p.bins = bins[:]

    def loadModel(self, checkpoint_folder):
        """
        Loads the ParticleSegmentationModel with a specified checkpoint.

        Input:
        - checkpoint_folder: Path to the folder containing model checkpoint.

        Output: None
        """
        def loadSamModel(checkpoint_folder):
            os.makedirs(checkpoint_folder, exist_ok=True)
            checkpoint_filename = "sam2.1_hiera_large.pt"  # SAM2
            CHECKPOINT_PATH = os.path.join(
                checkpoint_folder, checkpoint_filename)
            return CHECKPOINT_PATH

        CHECKPOINT_PATH = loadSamModel(checkpoint_folder)
        self.p = psa.ParticleSegmentationModel(
            self.imagePath, CHECKPOINT_PATH, self.Scaler.scalingFactor)

    def analyseParticles(self, checkpoint_folder, testing):
        """
        Analyzes particles in the image by generating masks using the model, and calculates analysis time.

        Inputs:
        - checkpoint_folder: Path to the model checkpoint.
        - testing: Boolean flag to enable test mode.

        Output: None
        """
        def calculateAnalysisTime(duration):
            total_seconds = duration.total_seconds()
            # Total seconds for the first calculation (Entire image)
            self.totalSeconds=total_seconds
            # minutes = int(total_seconds // 60)
            # seconds = total_seconds % 60
            # self.analysisTime = f"PT{minutes}M{seconds:.1f}S"

        self.loadModel(checkpoint_folder)
        if testing:
            self.p.testing_generate_mask()
        else:
            self.p.generate_mask()

        calculateAnalysisTime(self.p.getExecutionTime())
        self.p.setdiameter_threshold(self.diameter_threshold)
        self.csv_filename = os.path.join(
            self.folder_path, f"{self.sampleID}.csv")
        self.p.save_masks_to_csv(self.csv_filename)

    def savePsdData(self):
        """
        Saves particle size distribution (PSD) data to a text file.

        Input: None
        Output: Saves PSD data to a TXT file.
        """
        self.p.get_psd_data()
        self.distributions_filename = os.path.join(
            self.folder_path, f"{self.sampleID}_distribution.txt")
        self.p.save_psd_as_txt(self.sampleID, self.folder_path)
        print(f"--> PSD data saved as TXT file: {self.distributions_filename}")

    def saveResults(self, bins):
        """
        Saves particle segmentation results to CSV and distribution files after setting bins.

        Input:
        - bins: List of bin boundaries for the segmentation model.

        Output: Saves results to CSV and distribution files.
        """
        self.setBins(bins)
        if self.imageProcessor is None:
            raise ValueError("Image is not initialised")

        self.folder_path = self.imageProcessor.getImageFolder()
        self.csv_filename = os.path.join(
            self.folder_path, f"{self.sampleID}.csv")
        self.p.setdiameter_threshold(self.diameter_threshold)
        self.p.save_masks_to_csv(self.csv_filename)
        print(f"--> Masks saved to CSV file: {self.csv_filename}")

        self.savePsdData()

    def generateMasksForMeshing(self, testing):
        """
        Analyzes particles in the image by generating masks using the model for each segmented image
        and saves the resulting segment files to a corresponding folder.

        Inputs:
        - testing: Boolean flag to enable test mode.

        Output: None
        """

        def calculateTotalSeconds(duration):
            total_seconds = duration.total_seconds()
            self.meshingTotalSeconds += total_seconds

        # Step 1: Assign `self.meshingImageFolderPath`
        meshing_folder_name = "meshingImage"
        self.meshingImageFolderPath = os.path.join(self.folder_path, meshing_folder_name)

        # Step 2: Check if `meshingImage` folder exists
        if not os.path.exists(self.meshingImageFolderPath):
            print(f"Error: Folder '{meshing_folder_name}' not found at {self.folder_path}")
            return

        print(f"Meshing image folder path: {self.meshingImageFolderPath}")

        # Step 3: Assign `self.meshingSegmentsFolder`
        meshing_segment_folder_name = "meshingSegments"
        self.meshingSegmentsFolder = os.path.join(self.folder_path, meshing_segment_folder_name)

        # Create `meshingSegments` folder if it doesn't exist
        os.makedirs(self.meshingSegmentsFolder, exist_ok=True)
        print(f"Meshing segments folder path: {self.meshingSegmentsFolder}")

        # Step 4: List all image files in `meshingImageFolderPath` with natural sorting
        def natural_key(file_name):
            match = re.search(r'\d+', file_name)
            return int(match.group()) if match else float('inf')

        image_files = [
            os.path.join(self.meshingImageFolderPath, file)
            for file in sorted(os.listdir(self.meshingImageFolderPath),key=natural_key)
            if file.endswith(".png")  # Filter for PNG images
        ]

        if not image_files:
            print(f"No images found in {self.meshingImageFolderPath}")
            return

        print(f"Found {len(image_files)} images in {self.meshingImageFolderPath}")

        # Step 5: Loop through each image and process using the model
        for index, image_path in enumerate(image_files, start=1):
            print(f"Processing image: {image_path}")

            # Update the model's image path directly
            self.p.update_image_path(image_path)

            # Generate masks
            if testing:
                self.p.testing_generate_mask()
            else:
                self.p.generate_mask()

            # Step 6: Save masks to a corresponding CSV file
            self.p.setdiameter_threshold(self.diameter_threshold)
            csv_filename = os.path.join(self.meshingSegmentsFolder, f"meshing_{index}.csv")
            self.p.save_masks_to_csv(csv_filename)
            print(f"Segment file saved as: {csv_filename}")

            # Calculate execution time
            calculateTotalSeconds(self.p.getExecutionTime())

        print("Finished processing all images.")

    def setScalingFactor(self, scalingFactor):
        self.Scaler.setScalingFactor(scalingFactor)

    def formatResults(self):
        """
        Formats and displays analysis results, and saves formatted results as XML.

        Input: None
        Output: Prints formatted results and saves them to an XML file.
        """
        self.totArea = self.p.get_totalArea()
        print("-----------------------------------------------")
        print("Sample ID:", self.sampleID)
        print(f"Total Area: {self.totArea} um2")
        print(f"Total Area: {self.totArea / 100_000_000} cm2")
        print(f"Scaling Factor: {self.Scaler.scalingFactor} um/pixels")
        print(f"Scaling Number: {self.Scaler.scalingNumber} pixels")
        self.intensity = self.imageProcessor.getIntensity()
        print("Intensity:", self.intensity)
        print("Scaling Stamp:", self.Scaler.scalingStamp)
        print("Analysis Time:", self.analysisTime)
        print(f"Diameter Threshold: {self.p.diameter_threshold} um")
        print(f"Circularity Threshold: {self.p.circularity_threshold} um")
        print("-----------------------------------------------")
        print(f"CSV file: {self.csv_filename}")

        formatter = sa.sizeAnalysisModel(self.sampleID, self.csv_filename, self.distributions_filename,
                                         self.totArea, self.Scaler.scalingNumber,
                                         self.Scaler.scalingFactor, self.Scaler.scalingStamp,
                                         self.intensity, self.analysisTime, self.p.diameter_threshold,
                                         self.p.circularity_threshold)
        formatter.save_xml()

    def saveSegments(self):
        """
        Saves segment data as JSON for later use.

        Input: None
        Output: Saves segment data to JSON file.
        """
        self.p.setdiameter_threshold(self.diameter_threshold)
        self.json_filename = os.path.join(
            self.folder_path, f"{self.sampleID}_segments.txt")
        self.p.save_segments(self.json_filename)
        print(f"Saving segments in {self.json_filename}")

    def loadSegments(self, checkpoint_folder, bins):
        """
        Loads segments from a JSON file and saves them to CSV and distribution files, useful for non-GPU environments.

        Inputs:
        - checkpoint_folder: Path to the model checkpoint.
        - bins: List of bin boundaries for the segmentation model.

        Output: Saves segment data to CSV and distribution files.
        """
        try:
            self.setFolderPath()
            self.json_masks_filename = os.path.join(
                self.folder_path, f"{self.sampleID}_segments.txt")

            if not os.path.exists(self.json_masks_filename):
                raise FileNotFoundError(
                    f"The file {self.json_masks_filename} was not found.")

            self.loadModel(checkpoint_folder)
            self.setBins(bins)
            self.csv_filename = os.path.join(
                self.folder_path, f"{self.sampleID}.csv")
            self.p.setdiameter_threshold(self.diameter_threshold)
            self.p.save_segments_as_csv(
                self.json_masks_filename, self.csv_filename)
            self.savePsdData()

        except FileNotFoundError as e:
            raise e
        except Exception as e:
            raise Exception(f"An unexpected error occurred: {e}")

    def setFolderPath(self):
        """
        Sets the folder path for saving results, based on the initialized image processor.

        Input: None
        Output: Sets self.folder_path based on image folder path.
        """
        if self.imageProcessor is not None:
            self.folder_path = self.imageProcessor.getImageFolder()
        else:
            raise ValueError(
                "Image not initialized. Please ensure that 'imageProcessor' is properly initialized.")

    def crop_image(self):
        self.imageProcessor.cropImage()
        self.imagePath = self.imageProcessor.getImagePath()
        self.Scaler.updateScalingFactor(self.imageProcessor.getWidth())

    def evenLighting(self):
        self.imageProcessor.even_out_lighting()
        self.imagePath = self.imageProcessor.getImagePath()
        self.Scaler.updateScalingFactor(self.imageProcessor.getWidth())

    def overlayImage(self):
        """
        Calls the ImageProcessingModel's overlayImage function to overlay the same picture 10 times and 
        reducing the size of the image if it is bigger than 8MB

        Input: None
        Output: lighter PNG file and containing the same image overlayed 10 times
        """
        self.imageProcessor.overlayImage()
        self.imagePath = self.imageProcessor.getImagePath()
        self.Scaler.updateScalingFactor(self.imageProcessor.getWidth())
    def  meshingImage(self):
        self.imageProcessor.processImageWithMeshing()


    def plotBins(self):
        self.p.plotBins()

    def getAnalysisTime(self):
        total_seconds = self.totalSeconds+self.meshingTotalSeconds
        # Total seconds for the first calculation (Entire image)
        minutes = int(total_seconds // 60)
        seconds = total_seconds % 60
        self.analysisTime = f"PT{minutes}M{seconds:.1f}S"
        print(f"""The final analysing time is {self.analysisTime}""")
    def getSmallestAreaForFinalImage(self):
        """
        This function counts the number of data rows in a given file, ignoring the header row.

        Args:
        file_path (str): The path to the file.

        Returns:
        int: The number of data rows in the file.
        """
        particles=[]
        try:
            with open(self.csv_filename, 'r') as file:
                next(file)
                for line in file:
                    if line.strip():  # remove white space
                        area, perimeter, diameter, circularity = map(float, line.strip().split(','))
                        item = {
                            "area": area,
                            "perimeter": perimeter,
                            "diameter": diameter,
                            "circularity": circularity
                        }
                        particles.append(item)
            if len(particles) == 0:
                logger.error("There is no particles for minimumArea(ImageAnalysisModel) to be processed")
                return

            areas = [particle['area'] for particle in particles]
            sorted_areas = sorted(areas)
            self.mimumArea = format(max(float(sorted_areas[0] / 1000000), 0), '.8f')
            print(f'Minimu Area(ImageAnalysisModel) of the entire image analysis is :{self.mimumArea}')
            logger.info("Minimu Area(ImageAnalysisModel) of the entire image analysis is : {}", self.mimumArea)

        except Exception as e :
                logger.error("The give csv  file can  not be parsed due to {} error ",e)

    def getMeshingSegmentAreas(self):
        """
        Processes all CSV files in `self.meshingSegmentsFolder` and extracts particle areas.

        Args:
        None

        Returns:
        None
        """
        # Check if the folder exists
        if not os.path.exists(self.meshingSegmentsFolder):
            logger.error(f"Meshing segments folder {self.meshingSegmentsFolder} does not exist.")
            return

        # Get all CSV files in the folder
        segment_files = [
            os.path.join(self.meshingSegmentsFolder, file)
            for file in os.listdir(self.meshingSegmentsFolder)
            if file.endswith('.csv')
        ]

        if not segment_files:
            logger.error(f"No CSV files found in {self.meshingSegmentsFolder}")
            return

        # Process each CSV file
        for segment_file in sorted(segment_files):
            particles = []
            try:
                with open(segment_file, 'r') as file:
                    next(file)  # Skip the header row
                    for line in file:
                        if line.strip():  # Remove white space
                            # Parse the row
                            area, perimeter, diameter, circularity = map(float, line.strip().split(','))
                            item = {
                                "area": area,
                                "perimeter": perimeter,
                                "diameter": diameter,
                                "circularity": circularity
                            }
                            particles.append(item)

                if not particles:
                    logger.warning(f"No particles found in file {segment_file}")
                    continue

                # Extract areas and sort them
                areas = [particle['area'] for particle in particles]
                sorted_areas = sorted(areas)

                # Add the sorted areas for this file
                self.meshingSegmentAreas[segment_file] = sorted_areas
                print(f"Processed file {segment_file}: {len(sorted_areas)} particles")
                logger.info(f"Processed file {segment_file}: {len(sorted_areas)} particles")

            except Exception as e:
                logger.error(f"Failed to process file {segment_file} due to error: {e}")

        if not self.meshingSegmentAreas:
            logger.warning("No valid particle areas were found in any file.")




